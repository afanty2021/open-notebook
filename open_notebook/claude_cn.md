<<<<<<< HEAD
[æ ¹ç›®å½•](../CLAUDE.md) > **open_notebook**

# Open Notebook æ ¸å¿ƒæ¨¡å—

> æœ€åæ›´æ–°ï¼š2025-12-09 08:27:02

## æ¨¡å—èŒè´£

`open_notebook` æ˜¯æ•´ä¸ªé¡¹ç›®çš„æ ¸å¿ƒ Python åŒ…ï¼ŒåŒ…å«æ‰€æœ‰ä¸šåŠ¡é€»è¾‘ã€é¢†åŸŸæ¨¡å‹ã€AI å›¾é›†æˆã€æ•°æ®åº“æ“ä½œå’Œå·¥å…·å‡½æ•°ã€‚å®ƒè¢«è®¾è®¡ä¸ºç‹¬ç«‹çš„å¯é‡ç”¨åŒ…ï¼Œä¸º API å±‚æä¾›æ ¸å¿ƒåŠŸèƒ½æ”¯æŒã€‚

## å…¥å£ä¸å¯åŠ¨

### åŒ…åˆå§‹åŒ–
- **`__init__.py`** - åŒ…åˆå§‹åŒ–ï¼Œå¯¼å‡ºä¸»è¦ç±»å’Œå‡½æ•°
- **`config.py`** - å…¨å±€é…ç½®ç®¡ç†

### é…ç½®é¡¹
```python
# æ•°æ®æ–‡ä»¶å¤¹
DATA_FOLDER = "./data"
UPLOADS_FOLDER = f"{DATA_FOLDER}/uploads"
LANGGRAPH_CHECKPOINT_FILE = f"{DATA_FOLDER}/sqlite-db/checkpoints.sqlite"
TIKTOKEN_CACHE_DIR = f"{DATA_FOLDER}/tiktoken-cache"
```

## å¯¹å¤–æ¥å£

### ä¸»è¦å¯¼å‡º
```python
from open_notebook.domain import Notebook, Source, Note
from open_notebook.graphs import ChatGraph, TransformationGraph
from open_notebook.database import repository
from open_notebook.utils import context_builder
```

## å…³é”®ä¾èµ–ä¸é…ç½®

### å¤–éƒ¨ä¾èµ–
- **SurrealDB** - ä¸»æ•°æ®åº“
- **LangChain** - AI æ¡†æ¶
- **LangGraph** - AI å·¥ä½œæµ
- **Esperanto** - AI æ¨¡å‹æŠ½è±¡å±‚
- **Content Core** - å†…å®¹å¤„ç†
- **Podcast Creator** - æ’­å®¢ç”Ÿæˆ

### å†…éƒ¨ç»“æ„
```
open_notebook/
â”œâ”€â”€ database/         # æ•°æ®åº“å±‚
â”œâ”€â”€ domain/          # é¢†åŸŸæ¨¡å‹
â”œâ”€â”€ graphs/          # AI å·¥ä½œæµ
â”œâ”€â”€ utils/           # å·¥å…·å‡½æ•°
â”œâ”€â”€ plugins/         # æ’ä»¶ç³»ç»Ÿ
â”œâ”€â”€ config.py        # é…ç½®
â””â”€â”€ exceptions.py    # å¼‚å¸¸å®šä¹‰
```

## æ•°æ®æ¨¡å‹

### é¢†åŸŸæ¨¡å‹ (`domain/`)

1. **`base.py`** - åŸºç¡€æ¨¡å‹ç±»
   - `ObjectModel` - åŸºç¡€å¯¹è±¡æ¨¡å‹
   - `RecordModel` - è®°å½•æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰

2. **`models.py`** - AI æ¨¡å‹ç®¡ç†
   - `Model` - AI æ¨¡å‹å®šä¹‰
   - `ModelManager` - æ¨¡å‹ç®¡ç†å™¨
   - `DefaultModels` - é»˜è®¤æ¨¡å‹é…ç½®

3. **`notebook.py`** - ç¬”è®°æœ¬æ¨¡å‹
   - `Notebook` - ç¬”è®°æœ¬å®ä½“
   - ç¬”è®°æœ¬ä¸æºçš„å…³è”å…³ç³»

4. **`podcast.py`** - æ’­å®¢ç›¸å…³æ¨¡å‹
   - `Podcast` - æ’­å®¢å®ä½“
   - `SpeakerProfile` - è¯´è¯äººé…ç½®
   - `EpisodeProfile` - èŠ‚ç›®é…ç½®

5. **`transformation.py`** - å†…å®¹è½¬æ¢æ¨¡å‹
   - `Transformation` - è½¬æ¢è§„åˆ™
   - è½¬æ¢æ¨¡æ¿å’Œé…ç½®

### ç¤ºä¾‹æ¨¡å‹å®šä¹‰
```python
class Notebook(ObjectModel):
    table_name: ClassVar[str] = "notebook"
    name: str
    description: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
```

## AI å›¾ç³»ç»Ÿ (`graphs/`)

LangGraph å®ç°çš„ AI å·¥ä½œæµï¼š

1. **`chat.py`** - å¯¹è¯å›¾
   - å¤„ç†ç”¨æˆ·å¯¹è¯
   - ä¸Šä¸‹æ–‡ç®¡ç†
   - å“åº”ç”Ÿæˆ

2. **`ask.py`** - é—®ç­”å›¾
   - åŸºäºå†…å®¹çš„é—®ç­”
   - å¼•ç”¨ç”Ÿæˆ

3. **`transformation.py`** - å†…å®¹è½¬æ¢å›¾
   - è‡ªå®šä¹‰å†…å®¹å¤„ç†
   - æ‰¹é‡è½¬æ¢æ”¯æŒ

4. **`source.py`** - æºå¤„ç†å›¾
   - æ–‡æ¡£è§£æ
   - å†…å®¹æå–

5. **`tools.py`** - AI å·¥å…·é›†æˆ
   - æœç´¢å·¥å…·
   - è®¡ç®—å·¥å…·
   - å¤–éƒ¨ API é›†æˆ

### å›¾ä½¿ç”¨ç¤ºä¾‹
```python
from open_notebook.graphs.chat import ChatGraph

# åˆ›å»ºå¯¹è¯å›¾
chat_graph = ChatGraph(
    model_id="gpt-4",
    notebook_id="notebook_123"
)

# æ‰§è¡Œå¯¹è¯
response = await chat_graph.ainvoke({
    "messages": [("user", "è§£é‡Šè¿™ä¸ªæ¦‚å¿µ")]
})
```

## æ•°æ®åº“å±‚ (`database/`)

1. **`repository.py`** - æ•°æ®è®¿é—®å±‚
   - `repository` - ä¸»æ•°æ®åº“å®ä¾‹
   - `repo_query` - æŸ¥è¯¢å‡½æ•°
   - CRUD æ“ä½œå°è£…

2. **`migrate.py`** - æ•°æ®åº“è¿ç§»
   - è¿ç§»ç®¡ç†
   - ç‰ˆæœ¬æ§åˆ¶

3. **`async_migrate.py`** - å¼‚æ­¥è¿ç§»
   - å¼‚æ­¥è¿ç§»æ”¯æŒ

### æ•°æ®åº“æ“ä½œç¤ºä¾‹
```python
from open_notebook.database.repository import repo_query

# æŸ¥è¯¢ç¬”è®°æœ¬
notebooks = await repo_query(
    "SELECT * FROM notebook ORDER BY created_at DESC"
)

# åˆ›å»ºè®°å½•
await Notebook.create({
    "name": "æ–°ç¬”è®°æœ¬",
    "description": "æè¿°"
})
```

## å·¥å…·å‡½æ•° (`utils/`)

1. **`context_builder.py`** - ä¸Šä¸‹æ–‡æ„å»º
   - åŠ¨æ€ä¸Šä¸‹æ–‡ç»„è£…
   - éšç§çº§åˆ«æ§åˆ¶
   - Token ç®¡ç†

2. **`text_utils.py`** - æ–‡æœ¬å¤„ç†
   - æ–‡æœ¬æ¸…ç†
   - æ ¼å¼åŒ–

3. **`token_utils.py`** - Token ç®¡ç†
   - Token è®¡æ•°
   - æˆªæ–­ç­–ç•¥

4. **`version_utils.py`** - ç‰ˆæœ¬ç®¡ç†
   - ç‰ˆæœ¬æ£€æŸ¥
   - æ›´æ–°æç¤º

## æ’ä»¶ç³»ç»Ÿ (`plugins/`)

### æ’­å®¢æ’ä»¶
- **`podcasts.py`** - æ’­å®¢ç”Ÿæˆæ’ä»¶
  - å¤šè¯´è¯äººæ”¯æŒ
  - è‡ªå®šä¹‰æ¨¡æ¿
  - éŸ³é¢‘å¤„ç†é›†æˆ

## å¼‚å¸¸å¤„ç† (`exceptions.py`)

è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼š
- `OpenNotebookError` - åŸºç¡€å¼‚å¸¸
- `ModelNotFoundError` - æ¨¡å‹æœªæ‰¾åˆ°
- `InsufficientContextError` - ä¸Šä¸‹æ–‡ä¸è¶³
- `ProcessingError` - å¤„ç†é”™è¯¯

## æµ‹è¯•ä¸è´¨é‡

### æµ‹è¯•è¦†ç›–
- **å•å…ƒæµ‹è¯•** - å„æ¨¡å—ç‹¬ç«‹æµ‹è¯•
- **é›†æˆæµ‹è¯•** - æ¨¡å—é—´äº¤äº’æµ‹è¯•
- **è¦†ç›–ç‡** - ç›®æ ‡ 90%+

### æµ‹è¯•è¿è¡Œ
```bash
# è¿è¡Œæ ¸å¿ƒæ¨¡å—æµ‹è¯•
pytest tests/test_domain.py tests/test_graphs.py -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=open_notebook
```

## æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥
1. **æ¨¡å‹ç¼“å­˜** - AI æ¨¡å‹å®ä¾‹å¤ç”¨
2. **æŸ¥è¯¢ç¼“å­˜** - æ•°æ®åº“æŸ¥è¯¢ç»“æœç¼“å­˜
3. **å‘é‡ç¼“å­˜** - åµŒå…¥å‘é‡ç¼“å­˜

### å¼‚æ­¥å¤„ç†
- å…¨å¼‚æ­¥ API è®¾è®¡
- åå°ä»»åŠ¡é˜Ÿåˆ—
- æ‰¹é‡æ“ä½œæ”¯æŒ

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„é¢†åŸŸæ¨¡å‹
1. åœ¨ `domain/` åˆ›å»ºæ–°æ–‡ä»¶
2. ç»§æ‰¿ `ObjectModel` æˆ– `RecordModel`
3. å®šä¹‰ `table_name` å’Œå­—æ®µ

### åˆ›å»ºæ–°çš„ AI å›¾
1. åœ¨ `graphs/` åˆ›å»ºæ–°æ–‡ä»¶
2. ä½¿ç”¨ LangGraph æ„å»ºå·¥ä½œæµ
3. å®šä¹‰èŠ‚ç‚¹å’Œè¾¹

### æ·»åŠ å·¥å…·å‡½æ•°
1. åœ¨ `utils/` æ·»åŠ æ¨¡å—
2. ç¼–å†™çº¯å‡½æ•°
3. æ·»åŠ ç±»å‹æ³¨è§£

## å¸¸è§é—®é¢˜ (FAQ)

### Q: å¦‚ä½•è‡ªå®šä¹‰ AI æ¨¡å‹ï¼Ÿ
A: é€šè¿‡ `ModelManager` é…ç½®ï¼Œæ”¯æŒ 16+ æä¾›å•†ã€‚

### Q: å¦‚ä½•æ‰©å±•å†…å®¹è½¬æ¢ï¼Ÿ
A: åœ¨ `domain/transformation.py` æ·»åŠ æ–°ç±»å‹ï¼Œæˆ–åˆ›å»ºè‡ªå®šä¹‰å›¾ã€‚

### Q: æ•°æ®åº“è¿ç§»å¦‚ä½•å·¥ä½œï¼Ÿ
A: ä½¿ç”¨ SurrealDB çš„è¿ç§»ç³»ç»Ÿï¼Œæ–‡ä»¶åœ¨ `migrations/` ç›®å½•ã€‚

### Q: å¦‚ä½•ä¼˜åŒ–æ€§èƒ½ï¼Ÿ
A: ä½¿ç”¨ç¼“å­˜ã€è°ƒæ•´ä¸Šä¸‹æ–‡å¤§å°ã€é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

## ç›¸å…³æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæ¨¡å—
- `config.py` - é…ç½®ç®¡ç†
- `exceptions.py` - å¼‚å¸¸å®šä¹‰

### é¢†åŸŸå±‚
- `domain/base.py` - åŸºç¡€æ¨¡å‹
- `domain/models.py` - AI æ¨¡å‹
- `domain/notebook.py` - ç¬”è®°æœ¬
- `domain/podcast.py` - æ’­å®¢
- `domain/transformation.py` - è½¬æ¢

### AI å›¾
- `graphs/chat.py` - å¯¹è¯å›¾
- `graphs/ask.py` - é—®ç­”å›¾
- `graphs/transformation.py` - è½¬æ¢å›¾
- `graphs/tools.py` - å·¥å…·é›†

### æ•°æ®å±‚
- `database/repository.py` - æ•°æ®è®¿é—®
- `database/migrate.py` - è¿ç§»

### å·¥å…·
- `utils/context_builder.py` - ä¸Šä¸‹æ–‡æ„å»º
- `utils/token_utils.py` - Token ç®¡ç†

## å˜æ›´è®°å½• (Changelog)

### 2025-12-09 08:27:02
- ğŸ“ åˆ›å»ºæ ¸å¿ƒæ¨¡å—æ–‡æ¡£
- ğŸ—ï¸ æ•´ç†é¢†åŸŸæ¨¡å‹ç»“æ„
- ğŸ“Š æ·»åŠ  AI å›¾è¯´æ˜
- ğŸ”§ è¡¥å……æ‰©å±•æŒ‡å—

---

*æ­¤æ–‡æ¡£ç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œå¦‚éœ€æ›´æ–°è¯·å‚è€ƒé¡¹ç›®è´¡çŒ®æŒ‡å—*
=======
# Open Notebook Core Backend

The `open_notebook` module is the heart of the system: a multi-layer backend orchestrating AI-powered research workflows. It bridges domain models, asynchronous database operations, LangGraph-based content processing, and multi-provider AI model management.

## Purpose

Encapsulates the entire backend architecture:
1. **Data layer**: SurrealDB persistence with async CRUD and migrations
2. **Domain layer**: Research models (Notebook, Source, Note, etc.) with embedded relationships
3. **Workflow layer**: LangGraph state machines for content ingestion, chat, and transformations
4. **AI provisioning**: Multi-provider model management with smart fallback logic
5. **Support services**: Context building, tokenization, and utility functions

All components communicate through async/await patterns and use Pydantic for validation.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API / Streamlit UI                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                     â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Graphs (LangGraph)   â”‚   â”‚   Domain Models (Data)    â”‚
â”‚ - source.py (ingestion) â”‚   â”‚ - Notebook, Source, Note  â”‚
â”‚ - chat.py              â”‚   â”‚ - ChatSession, Asset       â”‚
â”‚ - ask.py (search)      â”‚   â”‚ - SourceInsight, Embeddingâ”‚
â”‚ - transformation.py    â”‚   â”‚ - Transformation, Settingsâ”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ - EpisodeProfile, Podcast â”‚
    â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                        â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  AI Module (Models)  â”‚      â”‚  Utils (Helpers)     â”‚
â”‚ - ModelManager       â”‚      â”‚ - ContextBuilder     â”‚
â”‚ - DefaultModels      â”‚      â”‚ - TokenUtils         â”‚
â”‚ - provision_langchainâ”‚      â”‚ - TextUtils          â”‚
â”‚ - Multi-provider AI  â”‚      â”‚ - VersionUtils       â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Database (SurrealDB)          â”‚
         â”‚ - repository.py (CRUD ops)     â”‚
         â”‚ - async_migrate.py (schema)    â”‚
         â”‚ - Configuration                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Catalog

### Core Layers

**See dedicated CLAUDE.md files for detailed patterns and usage:**

- **`database/`**: Async repository pattern (repo_query, repo_create, repo_upsert), connection pooling, and automatic schema migrations on API startup. See `database/CLAUDE.md`.

- **`domain/`**: Core data models using Pydantic with SurrealDB persistence. Two base classes: `ObjectModel` (mutable records with auto-increment IDs and embedding) and `RecordModel` (singleton configuration). Includes search functions (text_search, vector_search). See `domain/CLAUDE.md`.

- **`graphs/`**: LangGraph state machines for async workflows. Content ingestion (source.py), conversational agents (chat.py), search synthesis (ask.py), and transformations. Uses provision_langchain_model() for smart model selection with token-aware fallback. See `graphs/CLAUDE.md`.

- **`ai/`**: Centralized AI model lifecycle via Esperanto library. ModelManager factory with intelligent fallback (large context detection, type-specific defaults, config override). Supports 8+ providers (OpenAI, Anthropic, Google, Groq, Ollama, Mistral, DeepSeek, xAI). See `ai/CLAUDE.md`.

- **`utils/`**: Cross-cutting utilities: ContextBuilder (flexible context assembly from sources/notes/insights with token budgeting), TextUtils (truncation, cleaning), TokenUtils (GPT token counting), VersionUtils (schema compatibility). See `utils/CLAUDE.md`.

- **`podcasts/`**: Podcast generation models: SpeakerProfile (TTS voice config), EpisodeProfile (generation settings), PodcastEpisode (job tracking via surreal-commands). See `podcasts/CLAUDE.md`.

### Configuration & Exceptions

- **`config.py`**: Paths for data folder, uploads, LangGraph checkpoints, and tiktoken cache. Auto-creates directories.
- **`exceptions.py`**: Hierarchy of OpenNotebookError subclasses for database, file, network, authentication, and rate-limit failures.

## Data Flow: Content Ingestion

```
User uploads file/URL
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ source.py (LangGraph state machine) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. content_process()                â”‚
â”‚    - extract_content() from file/URLâ”‚
â”‚    - Use ContentSettings defaults    â”‚
â”‚    - speech_to_text model from DB   â”‚
â”‚                                     â”‚
â”‚ 2. save_source()                    â”‚
â”‚    - Update Source with full_text   â”‚
â”‚    - Preserve title if empty        â”‚
â”‚                                     â”‚
â”‚ 3. trigger_transformations()        â”‚
â”‚    - Parallel fan-out to each TXN   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ transformation.py (parallel)
         â”‚ - Apply prompt to source text
         â”‚ - Generate insights
         â”‚ - Auto-embed results
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Database Storage    â”‚
        â”‚ - Source.full_text  â”‚
        â”‚ - SourceInsight     â”‚
        â”‚ - Embeddings        â”‚
        â”‚ - (async job)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Fire-and-forget embeddings**: Source.vectorize() returns command_id without awaiting; embedding happens asynchronously via surreal-commands job system.

## Data Flow: Chat & Search

```
User message in chat
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ContextBuilder           â”‚
â”‚ - Select sources/notes   â”‚
â”‚ - Token budget limiting  â”‚
â”‚ - Priority weighting     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ chat.py or ask.py (LangGraph)    â”‚
â”‚ - Load context from above        â”‚
â”‚ - provision_langchain_model()    â”‚
â”‚   * Auto-upgrade for large text  â”‚
â”‚   * Apply model_id override      â”‚
â”‚ - Call LLM with context          â”‚
â”‚ - Store message in SqliteSaver   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM Response â”‚
    â”‚ (persisted)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Patterns Across Layers

### Async/Await Everywhere
All database operations, model provisioning, and graph execution are async. Mix with sync code only via `asyncio.run()` or LangGraph's async bridges (see graphs/CLAUDE.md for workarounds).

### Type-Driven Dispatch
Model types (language, embedding, speech_to_text, text_to_speech) drive factory logic in ModelManager. Domain model IDs encode their type: `notebook:uuid`, `source:uuid`, `note:uuid`.

### Smart Fallback Logic
`provision_langchain_model()` auto-detects large contexts (105K+ tokens) and upgrades to dedicated large_context_model. Falls back to default_chat_model if specific type not found.

### Fire-and-Forget Jobs
Time-consuming operations (embedding, podcast generation) return command_id immediately. Caller polls surreal-commands for status; no blocking.

### Embedding on Save
Domain models with `needs_embedding()=True` auto-generate embeddings in `save()`. Search functions (text_search, vector_search) use embeddings for semantic matching.

### Relationship Management
SurrealDB graph edges link entities: Notebookâ†’Source (has), Sourceâ†’Note (artifact), Noteâ†’Source (refers_to). See `relate()` in domain/base.py.

## Integration Points

**API startup** (`api/main.py`):
- AsyncMigrationManager.run_migration_up() on lifespan startup
- Ensures schema is current before handling requests

**Streamlit UI** (`pages/stream_app/`):
- Calls domain models directly to fetch/create notebooks, sources, notes
- Invokes graphs (chat, source, ask) via async wrapper
- Relies on API for migrations (deprecated check in UI)

**Background Jobs** (`surreal_commands`):
- Source.vectorize() submits async embedding job
- PodcastEpisode.get_job_status() polls job queue
- Decouples long-running operations from request flow

## Important Quirks & Gotchas

1. **Token counting rough estimate**: Uses cl100k_base encoding; may differ 5-10% from actual model
2. **Large context threshold hard-coded**: 105,000 token limit for large_context_model upgrade (not configurable)
3. **Async loop gymnastics in graphs**: ThreadPoolExecutor workaround for LangGraph sync nodes calling async functions (fragile)
4. **DefaultModels always fresh**: get_instance() bypasses singleton cache to pick up live config changes
5. **Polymorphic model.get()**: Resolves subclass from ID prefix; fails silently if subclass not imported
6. **RecordID string inconsistency**: repo_update() accepts both "table:id" format and full RecordID
7. **Snapshot profiles**: podcast profiles stored as dicts, so config updates don't affect past episodes
8. **No connection pooling**: Each repo_* creates new connection (adequate for HTTP but inefficient for bulk)
9. **Circular import guard**: utils imports domain; domain must not import utils (breaks on import)
10. **SqliteSaver shared location**: LangGraph checkpoints from LANGGRAPH_CHECKPOINT_FILE env var; all graphs use same file

## How to Add New Feature

**New data model**:
1. Create class inheriting from `ObjectModel` with `table_name` ClassVar
2. Define Pydantic fields and validators
3. Override `needs_embedding()` if searchable
4. Add custom methods for domain logic (get_X, add_to_Y)
5. Register in domain/__init__.py exports

**New workflow**:
1. Create state machine in graphs/WORKFLOW.py using StateGraph
2. Import domain models and provision_langchain_model()
3. Define nodes as async functions taking State, returning dict
4. Compile with graph.compile()
5. Invoke from API endpoint or Streamlit page

**New AI model type**:
1. Add type string to Model class
2. Add AIFactory.create_* method in Esperanto
3. Handle in ModelManager.get_model()
4. Add DefaultModels field + getter

## Key Dependencies

- **surrealdb**: AsyncSurreal client, RecordID type
- **pydantic**: Validation, field_validator
- **langgraph**: StateGraph, Send, SqliteSaver, async/sync bridging
- **langchain_core**: Messages, OutputParser, RunnableConfig
- **esperanto**: Multi-provider AI model abstraction (OpenAI, Anthropic, Google, Groq, Ollama, etc.)
- **content-core**: File/URL content extraction
- **ai_prompter**: Jinja2 template rendering for prompts
- **surreal_commands**: Async job queue for embeddings, podcast generation
- **loguru**: Structured logging throughout
- **tiktoken**: GPT token encoding for context window estimation

## Codebase Statistics

- **Modules**: 6 core layers + support services
- **Async operations**: Database, AI provisioning, graph execution, embedding, job tracking
- **Supported AI providers**: 8+ (OpenAI, Anthropic, Google, Groq, Ollama, Mistral, DeepSeek, xAI, OpenRouter)
- **Domain models**: Notebook, Source, Note, SourceInsight, SourceEmbedding, ChatSession, Asset, Transformation, ContentSettings, EpisodeProfile, SpeakerProfile, PodcastEpisode
- **Graph workflows**: 6 (source, chat, source_chat, ask, transformation, prompt)
>>>>>>> upstream/main
