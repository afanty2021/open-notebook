<<<<<<< HEAD
[æ ¹ç›®å½•](../../CLAUDE.md) > **prompts**

# æç¤ºæ¨¡æ¿æ¨¡å—

## æ¨¡å—èŒè´£

æç¤ºæ¨¡æ¿æ¨¡å—è´Ÿè´£ç®¡ç†æ‰€æœ‰ AI äº¤äº’çš„æç¤ºè¯æ¨¡æ¿ã€‚è¿™äº›æ¨¡æ¿ä½¿ç”¨ Jinja2 æ¨¡æ¿å¼•æ“ï¼Œæ”¯æŒåŠ¨æ€å˜é‡æ³¨å…¥ï¼Œä¸ºä¸åŒçš„ AI äº¤äº’åœºæ™¯æä¾›ç»“æ„åŒ–ã€ä¸€è‡´çš„æç¤ºè¯ï¼Œç¡®ä¿ AI å“åº”çš„è´¨é‡å’Œä¸€è‡´æ€§ã€‚

## å…¥å£ä¸ç»“æ„

### ç›®å½•ç»“æ„
```
prompts/
â”œâ”€â”€ chat.jinja           # é€šç”¨èŠå¤©æç¤ºæ¨¡æ¿
â””â”€â”€ source_chat.jinja    # æºæ–‡ä»¶ä¸“ç”¨èŠå¤©æ¨¡æ¿
```

## æç¤ºæ¨¡æ¿è¯¦è§£

### 1. chat.jinja - é€šç”¨èŠå¤©æç¤ºæ¨¡æ¿

#### è®¾è®¡ç†å¿µ
ä¸ºè®¤çŸ¥å­¦ä¹ åŠ©æ‰‹æä¾›ç³»ç»Ÿè§’è‰²å®šä¹‰å’Œæ“ä½œæŒ‡å—ï¼Œå¸®åŠ©ç”¨æˆ·ä¸ç ”ç©¶æ–‡æ¡£è¿›è¡Œæœ‰æ„ä¹‰çš„å¯¹è¯ã€‚

#### æ ¸å¿ƒç»„ä»¶

##### ç³»ç»Ÿè§’è‰²å®šä¹‰
```
You are a cognitive study assistant that helps users research and learn
by engaging in focused discussions about documents in their workspace.
```

##### èƒ½åŠ›è¯´æ˜
- è®¿é—®é¡¹ç›®ä¿¡æ¯å’Œé€‰å®šæ–‡æ¡£
- ä¿æŒå­¦æœ¯ä¸¥è°¨æ€§çš„è‡ªç„¶å¯¹è¯
- åŸºäºä¸Šä¸‹æ–‡æä¾›å‡†ç¡®å›ç­”

##### æ“ä½œæ–¹æ³•
1. **è¯†åˆ«æŸ¥è¯¢ä¸Šä¸‹æ–‡**ï¼šç†è§£ç”¨æˆ·æ„å›¾
2. **åˆ†æä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šåˆ©ç”¨æä¾›çš„æ–‡æ¡£
3. **éµå¾ªå¼•ç”¨è§„èŒƒ**ï¼šæ­£ç¡®å¼•ç”¨æ¥æº

##### åŠ¨æ€å†…å®¹æ³¨å…¥
```jinja2
{% if notebook %}
# PROJECT INFORMATION
{{notebook}}
=======
# Prompts Module

Jinja2 prompt templates for multi-provider AI workflows in Open Notebook.

## Purpose

Centralized prompt repository using `ai_prompter` library to:
1. Separate prompt engineering from Python application logic
2. Provide reusable Jinja2 templates with variable injection
3. Support multi-stage prompt chains (orchestrated by LangGraph workflows)
4. Ensure consistency across similar workflows (chat, search, content generation)

## Architecture Overview

**Template Organization by Workflow**:
- **`ask/`**: Multi-stage search synthesis (entry â†’ query_process â†’ final_answer)
- **`chat/`**: Conversational agent with notebook context (system prompt only)
- **`source_chat/`**: Source-focused chat with insight injection (system prompt only)
- **`podcast/`**: Podcast generation pipeline (outline â†’ transcript)

**Rendering Pattern** (all workflows):
```python
from ai_prompter import Prompter

# Load template + render with variables
system_prompt = Prompter(prompt_template="ask/entry", parser=parser).render(
    data=state
)

# Then invoke LLM
model = await provision_langchain_model(system_prompt, ...)
response = await model.ainvoke(system_prompt)
```

See detailed workflow integration in `open_notebook/graphs/CLAUDE.md` for how each template fits into chat.py, ask.py, source_chat.py.

## Prompt Engineering Patterns

### 1. Multi-Stage Chain (Ask Workflow)

Three-template chain for intelligent search:

```
entry.jinja (user question â†’ search strategy)
    â†“
query_process.jinja (run each search, generate sub-answer)
    â†“ (multiple parallel)
final_answer.jinja (synthesize all results into final response)
```

**Key pattern**: `entry.jinja` generates JSON-structured reasoning (via PydanticOutputParser). Each `query_process.jinja` invocation receives one search term + retrieved results. `final_answer.jinja` combines all answers with proper source citation.

### 2. Conditional Variable Injection (Podcast Workflow)

Templates accept optional variables for context assembly:

```jinja
{% if notebook %}
# PROJECT INFORMATION
{{ notebook }}
>>>>>>> upstream/main
{% endif %}

{% if context %}
# CONTEXT
<<<<<<< HEAD
The user has selected this context to help you with your response:
{{context}}
{% endif %}
```

##### å¼•ç”¨è§„èŒƒ
- ä½¿ç”¨ `[document_id]` æ ¼å¼å¼•ç”¨æ–‡æ¡£
- ID æ ¼å¼ï¼š`type:randomstring`ï¼ˆå¦‚ `note:xyz`, `source:abc`ï¼‰
- ä¸å¾—ç¼–é€ æ–‡æ¡£ ID
- å¿…é¡»ä½¿ç”¨å®Œæ•´çš„ ID åŒ…æ‹¬ç±»å‹å‰ç¼€

##### ç¤ºä¾‹è¯´æ˜
æä¾›æ¸…æ™°çš„å¼•ç”¨ç¤ºä¾‹ï¼š
```
User: Can you tell me more about "Deep Learning"?
Assistant: Deep learning is a subset of machine learning... [note:iuiodadalknda]
```

### 2. source_chat.jinja - æºæ–‡ä»¶ä¸“ç”¨èŠå¤©æ¨¡æ¿

#### è®¾è®¡ç›®æ ‡
ä¸“é—¨é’ˆå¯¹å•ä¸ªæºæ–‡ä»¶çš„æ·±åº¦åˆ†æå’Œè®¨è®ºï¼Œæä¾›æ›´èšç„¦çš„å¯¹è¯ä½“éªŒã€‚

#### ç‰¹è‰²åŠŸèƒ½

##### ä¸“ä¸šåŒ–è§’è‰²
```
You are a specialized research assistant focused on helping users
deeply understand and analyze a specific source document.
```

##### æºæ–‡ä»¶ä¿¡æ¯å±•ç¤º
```jinja2
{% if source %}
**Source ID:** {{ source.id }}
**Title:** {{ source.title or "No title" }}

{% if source.topics %}
**Topics:** {{ source.topics | join(", ") }}
{% endif %}
{% endif %}
```

##### ä¸Šä¸‹æ–‡æ„ŸçŸ¥
- æ˜¾ç¤ºæºæ–‡ä»¶åŸºæœ¬ä¿¡æ¯
- å±•ç¤ºä¸»é¢˜æ ‡ç­¾
- æä¾›æºæ–‡ä»¶ä¸“å±ä¸Šä¸‹æ–‡

##### å¼•ç”¨æ ¼å¼ä¼˜åŒ–
- æºå†…å®¹å¼•ç”¨ï¼š`[source:id]`
- æ´å¯Ÿå¼•ç”¨ï¼š`[insight:id]`
- å¼ºè°ƒä½¿ç”¨çœŸå® ID

##### å¯¹è¯ç„¦ç‚¹
ä¸“æ³¨äºï¼š
- ç†è§£å¤æ‚æ¦‚å¿µ
- å»ºç«‹å†…å®¹å…³è”
- æ¢ç´¢æ·±å±‚å«ä¹‰
- æå‡ºè·Ÿè¿›é—®é¢˜

## æ¨¡æ¿å¼€å‘æŒ‡å—

### 1. åˆ›å»ºæ–°æ¨¡æ¿

#### åŸºæœ¬ç»“æ„
```jinja2
# SYSTEM ROLE
[ç³»ç»Ÿè§’è‰²å®šä¹‰]

# CAPABILITIES
[èƒ½åŠ›åˆ—è¡¨]

# YOUR OPERATING METHOD
[æ“ä½œæŒ‡å—]

{% if dynamic_content %}
# DYNAMIC SECTION
{{ dynamic_content }}
{% endif %}

# SPECIFIC INSTRUCTIONS
[å…·ä½“æŒ‡ä»¤]
```

#### å˜é‡ä½¿ç”¨
```jinja2
{% if variable %}
# SECTION TITLE
{{ variable }}
{% endif %}

{% for item in list %}
- {{ item }}
{% endfor %}

{{ variable | filter }}
```

### 2. æœ€ä½³å®è·µ

##### æ¸…æ™°çš„ç»“æ„
- ä½¿ç”¨æ ‡é¢˜åˆ†å±‚
- é€»è¾‘æ¸…æ™°çš„ç»„ç»‡
- æ˜ç¡®çš„æŒ‡ä»¤è¯´æ˜

##### æ¡ä»¶æ¸²æŸ“
```jinja2
{% if condition %}
<!-- æ¡ä»¶æ»¡è¶³æ—¶æ˜¾ç¤º -->
{% else %}
<!-- æ¡ä»¶ä¸æ»¡è¶³æ—¶æ˜¾ç¤º -->
{% endif %}
```

##### å¾ªç¯æ¸²æŸ“
```jinja2
{% for item in items %}
{{ loop.index }}. {{ item }}
{% endfor %}
```

##### è¿‡æ»¤å™¨ä½¿ç”¨
```jinja2
{{ text | upper }}
{{ list | join(", ") }}
{{ date | strftime("%Y-%m-%d") }}
```

### 3. å˜é‡å‘½åè§„èŒƒ
- ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿
- æè¿°æ€§åç§°
- é¿å…å†²çª

```python
# å¥½çš„ä¾‹å­
user_query
selected_context
source_document
conversation_history

# é¿å…ä½¿ç”¨
content
data
info
```

## æ¨¡æ¿ä½¿ç”¨

### 1. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from jinja2 import Environment, FileSystemLoader

# åŠ è½½æ¨¡æ¿ç¯å¢ƒ
env = Environment(loader=FileSystemLoader("prompts/"))
template = env.get_template("chat.jinja")

# æ¸²æŸ“æ¨¡æ¿
prompt = template.render(
    notebook=notebook_info,
    context=selected_context,
    user_query=query_text
)
```

### 2. åŠ¨æ€å˜é‡ä¼ é€’

```python
# ä¼ é€’å¤æ‚å¯¹è±¡
template.render(
    source={
        "id": "source:123",
        "title": "Research Paper",
        "topics": ["AI", "ML", "NLP"]
    },
    context=search_results,
    user_message="Explain the methodology"
)
```

### 3. æ¨¡æ¿ç»§æ‰¿

åˆ›å»ºåŸºç¡€æ¨¡æ¿ï¼š
```jinja2
{# base.jinja #}
# SYSTEM ROLE
You are a helpful AI assistant.

{% block content %}
{% endblock %}

# ADDITIONAL INSTRUCTIONS
{% block instructions %}
{% endblock %}
```

ç»§æ‰¿ä½¿ç”¨ï¼š
```jinja2
{# specialized.jinja #}
{% extends "base.jinja" %}

{% block content %}
# SPECIALIZED CAPABILITIES
- Capability 1
- Capability 2
{% endblock %}
```

## æ¨¡æ¿æµ‹è¯•

### 1. å•å…ƒæµ‹è¯•
```python
def test_chat_template():
    template = env.get_template("chat.jinja")

    # æµ‹è¯•åŸºæœ¬æ¸²æŸ“
    output = template.render(
        notebook="Test Notebook",
        context="Test Context"
    )

    assert "Test Notebook" in output
    assert "Test Context" in output
```

### 2. è¾¹ç•Œæ¡ä»¶æµ‹è¯•
- ç©ºå˜é‡å¤„ç†
- None å€¼å¤„ç†
- å¤§å‹æ•°æ®æ¸²æŸ“
- ç‰¹æ®Šå­—ç¬¦è½¬ä¹‰

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡æ¿ç¼“å­˜
```python
# å¯ç”¨è‡ªåŠ¨ç¼“å­˜
env = Environment(
    loader=FileSystemLoader("prompts/"),
    autoescape=True,
    cache_size=100
)
```

### 2. é¢„ç¼–è¯‘æ¨¡æ¿
```python
# é¢„ç¼–è¯‘å¸¸ç”¨æ¨¡æ¿
compiled_templates = {}
for name in ["chat", "source_chat"]:
    template = env.get_template(f"{name}.jinja")
    compiled_templates[name] = template
```

## å®‰å…¨è€ƒè™‘

### 1. è‡ªåŠ¨è½¬ä¹‰
```python
# å¯ç”¨ HTML è‡ªåŠ¨è½¬ä¹‰
env = Environment(
    loader=FileSystemLoader("prompts/"),
    autoescape=True
)
```

### 2. è¾“å…¥éªŒè¯
- éªŒè¯å˜é‡ç±»å‹
- é™åˆ¶å˜é‡é•¿åº¦
- è¿‡æ»¤æ•æ„Ÿå†…å®¹

## æœªæ¥æ‰©å±•

### å¯èƒ½çš„æ–°æ¨¡æ¿
1. **transformation.jinja** - å†…å®¹è½¬æ¢æç¤º
2. **summarization.jinja** - æ‘˜è¦ç”Ÿæˆæç¤º
3. **question_generation.jinja** - é—®é¢˜ç”Ÿæˆæç¤º
4. **citation.jinja** - å¼•ç”¨æ ¼å¼åŒ–æç¤º
5. **podcast_script.jinja** - æ’­å®¢è„šæœ¬ç”Ÿæˆæç¤º

### æ¨¡æ¿åŠŸèƒ½å¢å¼º
1. **å¤šè¯­è¨€æ”¯æŒ** - å›½é™…åŒ–æ¨¡æ¿
2. **ä¸ªæ€§åŒ–è°ƒæ•´** - åŸºäºç”¨æˆ·åå¥½çš„æ¨¡æ¿
3. **A/B æµ‹è¯•** - ä¸åŒç‰ˆæœ¬çš„æ•ˆæœå¯¹æ¯”
4. **æ¨¡æ¿ç‰ˆæœ¬æ§åˆ¶** - è·Ÿè¸ªæ¨¡æ¿å˜æ›´

## ç›¸å…³æ–‡ä»¶æ¸…å•

```
prompts/
â”œâ”€â”€ chat.jinja           # é€šç”¨èŠå¤©æ¨¡æ¿
â””â”€â”€ source_chat.jinja    # æºæ–‡ä»¶èŠå¤©æ¨¡æ¿
```

## å˜æ›´è®°å½• (Changelog)

### 2025-12-09 08:29:13
- ğŸ“ åˆ›å»ºæç¤ºæ¨¡æ¿æ¨¡å—æ–‡æ¡£
- ğŸ“š è¯¦ç»†åˆ†æç°æœ‰æ¨¡æ¿ç»“æ„
- ğŸ¯ æä¾›æ¨¡æ¿å¼€å‘æŒ‡å—
- ğŸ”§ è¡¥å……æµ‹è¯•å’Œå®‰å…¨å»ºè®®
=======
{{ context }}
{% endif %}
```

Enabled by Jinja2's conditional blocks. Critical for podcast outline (handles list or string context) and source_chat (injects variable notebook/insight data).

### 3. Repeated Emphasis on Citation Format (Ask & Chat)

All response-generating templates emphasize source citation rules:
- Document ID syntax: `[source:id]`, `[note:id]`, `[insight:id]`
- "Do not make up document IDs" repeated multiple times
- Example citations provided inline

**Rationale**: LLMs naturally hallucinate citations without explicit guidance; repetition + examples reduce hallucination.

### 4. Format Instructions Delegation

Templates accept external `{{ format_instructions }}` variable:

```jinja
# OUTPUT FORMATTING
{{ format_instructions }}
```

Allows caller to inject JSON schema, XML format, or other output constraints without modifying template. Decouples prompt from output format evolution.

### 5. JSON Output with Extended Thinking Support

Podcast templates include extended thinking pattern:

```jinja
IMPORTANT OUTPUT FORMAT:
- If you use extended thinking with <think> tags, put ALL your reasoning inside <think></think> tags
- Put the final JSON output OUTSIDE and AFTER any <think> tags
```

Guides models with extended thinking capability to separate reasoning from output (cleaner parsing downstream).

## File Catalog

**`ask/` - Search Synthesis Pipeline**:
- **entry.jinja**: Analyzes user question, generates search strategy with JSON output (term + instructions per search)
- **query_process.jinja**: Accepts one search term + retrieved results, generates sub-answer with citations
- **final_answer.jinja**: Combines all sub-answers into coherent final response, enforces source citation

**`chat/` - Conversational Agent**:
- **system.jinja**: Single system prompt for general chat. Uses conditional blocks for optional notebook context. Emphasizes citation format.

**`source_chat/` - Source-Focused Chat**:
- **system.jinja**: Single system prompt for source-specific discussion. Injects source metadata (ID, title, topics) + selected context. Conditional blocks for optional notebook/context data.

**`podcast/` - Podcast Generation**:
- **outline.jinja**: Takes briefing + content + speaker profiles (list support via Jinja2 for-loop). Generates JSON outline with segments (name, description, size).
- **transcript.jinja**: Takes outline + segment index + optional existing transcript. Generates JSON dialogue array (speaker name + dialogue). Iterates speakers with for-loop.

## Key Dependencies

- **ai_prompter**: Prompter class for Jinja2 template rendering with optional OutputParser binding
- **Jinja2** (transitive via ai_prompter): Template syntax (if/for, filters, variable interpolation)
- **No external AI calls**: Templates are pure text; LLM invocation happens in calling code (graphs/)

## How to Add New Template

1. **Create subdirectory** in `prompts/` matching workflow name (e.g., `prompts/new_workflow/`)
2. **Define .jinja file(s)** with Jinja2 syntax:
   - Use `{{ variable_name }}` for scalar injection
   - Use `{% if condition %} ... {% endif %}` for optional sections
   - Use `{% for item in list %} ... {% endfor %}` for iteration
3. **Document template variables** as inline comments (follow existing templates)
4. **Reference in calling code** (graphs/):
   ```python
   from ai_prompter import Prompter
   prompt = Prompter(prompt_template="new_workflow/template_name").render(data=context_dict)
   ```
5. **If structured output needed**: Pass `parser=PydanticOutputParser(...)` to Prompter
6. **Document in graphs/CLAUDE.md** how new template fits into workflow chain

## Important Quirks & Gotchas

1. **Template path syntax**: Uses forward slashes without `.jinja` extension in Prompter. `"ask/entry"` maps to `/prompts/ask/entry.jinja`
2. **Variable key convention**: All data passed as `data=dict` arg to `.render()`. Template accesses variables directly (e.g., `{{ question }}`). Ensure dict keys match template variable names.
3. **OutputParser binding**: When using PydanticOutputParser, Prompter auto-injects `{{ format_instructions }}` into template. If template doesn't have this placeholder, parser is ignored.
4. **Jinja2 whitespace sensitivity**: Template indentation doesn't affect output, but raw newlines do. Use explicit `\n` or trim filters if output formatting matters.
5. **Conditional blocks are loose**: Jinja2 if-condition evaluates any truthy value (non-empty string, list, dict). `{% if variable %}` is False for empty string/"" but True for any non-empty content.
6. **For-loop list assumption**: Templates using `{% for item in list %}` don't validate list type. If caller passes string instead of list, iteration happens character-by-character (bug risk).
7. **No template composition/inheritance**: Templates are flat (no `{% extends %}` or `{% include %}`). Each workflow keeps templates independent to avoid coupling.
8. **Citation ID format is caller's responsibility**: Templates emphasize citation rules but don't validate. If caller returns wrong ID format, template can't catch it upstream.
9. **Parser extraction happens post-render**: OutputParser.parse() is called AFTER `.render()` returns string. If template has syntax errors, render fails before parsing logic runs.
10. **Template cache**: Prompter likely caches loaded templates. File edits require app restart if using cached instance.

## Testing Patterns

**Manual render test**:
```python
from ai_prompter import Prompter

prompt = Prompter(prompt_template="ask/entry").render(
    data={"question": "What is RAG?"}
)
print(prompt)  # Inspect Jinja2 output before sending to LLM
```

**With parser**:
```python
from pydantic import BaseModel
from langchain_core.output_parsers.pydantic import PydanticOutputParser

class Strategy(BaseModel):
    reasoning: str
    searches: list

parser = PydanticOutputParser(pydantic_object=Strategy)
prompt = Prompter(prompt_template="ask/entry", parser=parser).render(
    data={"question": "..."}
)
# prompt now includes {{ format_instructions }} substitution
```

**Integration test** (invoke full graph):
See `open_notebook/graphs/ask.py` for how entry.jinja is invoked inside ask_graph workflow.

## Reference Documentation

- **Jinja2 syntax guide**: See existing templates for for-loop, if-conditional, variable interpolation patterns
- **Graph integration**: `open_notebook/graphs/CLAUDE.md` documents which template is used in which workflow
- **Sub-directory CLAUDE.md files**: `ask/CLAUDE.md`, `chat/CLAUDE.md`, `podcast/CLAUDE.md` (if created) provide template-specific implementation notes
>>>>>>> upstream/main
